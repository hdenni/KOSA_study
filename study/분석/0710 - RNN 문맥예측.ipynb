{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important;} *{font-family:'Consolas'; font-size:14;} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important;} *{font-family:'Consolas'; font-size:14;} </style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문맥을 예측해서 다음 단어 예측해보기 - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample text 출처: https://ivynet.co.kr/lorem-ipsum-korean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "브랜드(Brand)는 어떤 경제적인 생산자를 구별하는 지각된 이미지와 경험의 집합이며 보다 좁게는 어떤 상품이나 회사를 나타내는 상표, 표지이다.\\n 숫자, 글자, 글자체, 간략화된 이미지인 로고, 색상, 구호를 포함한다.\\n 브랜드는 특히 기업의 무형자산으로 소비자와 시장에서 그 기업을 나타내는 가치를 나태낸다.\\n 마케팅, 광고, 홍보, 제품 디자인 등에 직접 사용되며, 문화나 경제에 있어 현대의 산업소비 사회를 나타내는 중요한 요소이기도 하다.\\n 많은 연구자들은 브랜드가 일단 리더십을 얻게 되면 그것을 수십 년간 유지해왔다고 주장하고 있다.\\n 경영자들이 브랜드의 리더십 유지를 중요하게 생각하는 데는 몇 가지 이유가 있다.\\n 첫째, 오래된 브랜드가 고객의 관심을 받기 때문이다.\\n 오래된 브랜드는 상대적으로 마케팅 비용을 적게 들여도, 프리미엄 가격을 받을 수 있다.\\n 이와는 대조적으로 새 브랜드나 시장이 작은 브랜드는 시장 진출과 인지도 획득을 위해 많은 마케팅 비용을 들여야 한다.\\n 둘째, 마켓리더는 규모의 경제를 이용할 수 있다.\\n 따라서 좁은 틈새시장을 확보한 경쟁상대에 비해 능률적인 경영정책으로 더 높은 수익을 누릴 수 있다.\\n 셋째, 이미 특정 부문에서 우위를 차지하고 있는 마켓리더는 새로운 관련 부문으로 프랜차이지를 확대할 수 있으며, 새 부문에서도 쉽게 우위를 점할 수 있다.\\n 반면 리더가 되지 못한 회사들의 경우, 매년 수천 개의 새 브랜드를 출시하고 있지만 대부분이 시장에서 자취도 없이 사라진다.\\n \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.text import Tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])\n",
    "encoded = t.texts_to_sequences([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n",
    "# 케라스의 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n",
    "# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야 한다.\n",
    "\n",
    "print(\"단어 집합의 크기 : %d\" % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수: 11\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in text.split('\\n'): # \\n을 기준으로 문장 토큰화\n",
    "    encoded = t.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "print(\"훈련 데이터의 개수: %d\" % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = max(len(l) for l in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sequences = pad_sequences(sequences, maxlen=max(len(l) for l in sequences), padding='pre')\n",
    "# pad_sequences()는 모든 데이터에 대해서 0을 추가하여 길이를 맞춰줌\n",
    "# maxlen의 값을 6으로 주면 모든 데이터의 길이를 sequences중 가장 길이가 긴 길이에 맞게 맞춰주며, padding의 인자로 'pre'를 주면 길이가 maxlen보다 짧은 데이터의 \"앞\"을 0으로 채움 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  0,  2,  3,  1],\n",
       "       [ 0,  0,  2,  3,  1,  4],\n",
       "       [ 0,  2,  3,  1,  4,  5],\n",
       "       [ 0,  0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  6,  1,  7],\n",
       "       [ 0,  0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  0,  8,  1,  9],\n",
       "       [ 0,  0,  8,  1,  9, 10],\n",
       "       [ 0,  8,  1,  9, 10,  1],\n",
       "       [ 8,  1,  9, 10,  1, 11]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sequences = np.array(sequences)\n",
    "X = sequences[:, :-1] # 리스트의 마지막 열을 제외하고 저장\n",
    "y = sequences[:, -1] # 리스트의 마지막 열만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 훈련데이터를 훈련 시키기 전에 y에 대해서 원-핫 인코딩을 수행\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  2,  3],\n",
       "       [ 0,  0,  2,  3,  1],\n",
       "       [ 0,  2,  3,  1,  4],\n",
       "       [ 0,  0,  0,  0,  6],\n",
       "       [ 0,  0,  0,  6,  1],\n",
       "       [ 0,  0,  0,  0,  8],\n",
       "       [ 0,  0,  0,  8,  1],\n",
       "       [ 0,  0,  8,  1,  9],\n",
       "       [ 0,  8,  1,  9, 10],\n",
       "       [ 8,  1,  9, 10,  1]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "Epoch 1/200\n",
      "11/11 - 1s - loss: 2.5149 - accuracy: 0.0000e+00\n",
      "Epoch 2/200\n",
      "11/11 - 0s - loss: 2.5015 - accuracy: 0.0000e+00\n",
      "Epoch 3/200\n",
      "11/11 - 0s - loss: 2.4884 - accuracy: 0.0909\n",
      "Epoch 4/200\n",
      "11/11 - 0s - loss: 2.4756 - accuracy: 0.1818\n",
      "Epoch 5/200\n",
      "11/11 - 0s - loss: 2.4629 - accuracy: 0.2727\n",
      "Epoch 6/200\n",
      "11/11 - 0s - loss: 2.4504 - accuracy: 0.1818\n",
      "Epoch 7/200\n",
      "11/11 - 0s - loss: 2.4376 - accuracy: 0.2727\n",
      "Epoch 8/200\n",
      "11/11 - 0s - loss: 2.4247 - accuracy: 0.3636\n",
      "Epoch 9/200\n",
      "11/11 - 0s - loss: 2.4113 - accuracy: 0.4545\n",
      "Epoch 10/200\n",
      "11/11 - 0s - loss: 2.3975 - accuracy: 0.4545\n",
      "Epoch 11/200\n",
      "11/11 - 0s - loss: 2.3830 - accuracy: 0.4545\n",
      "Epoch 12/200\n",
      "11/11 - 0s - loss: 2.3678 - accuracy: 0.4545\n",
      "Epoch 13/200\n",
      "11/11 - 0s - loss: 2.3518 - accuracy: 0.4545\n",
      "Epoch 14/200\n",
      "11/11 - 0s - loss: 2.3350 - accuracy: 0.4545\n",
      "Epoch 15/200\n",
      "11/11 - 0s - loss: 2.3171 - accuracy: 0.4545\n",
      "Epoch 16/200\n",
      "11/11 - 0s - loss: 2.2982 - accuracy: 0.4545\n",
      "Epoch 17/200\n",
      "11/11 - 0s - loss: 2.2782 - accuracy: 0.4545\n",
      "Epoch 18/200\n",
      "11/11 - 0s - loss: 2.2571 - accuracy: 0.4545\n",
      "Epoch 19/200\n",
      "11/11 - 0s - loss: 2.2347 - accuracy: 0.4545\n",
      "Epoch 20/200\n",
      "11/11 - 0s - loss: 2.2112 - accuracy: 0.4545\n",
      "Epoch 21/200\n",
      "11/11 - 0s - loss: 2.1864 - accuracy: 0.4545\n",
      "Epoch 22/200\n",
      "11/11 - 0s - loss: 2.1605 - accuracy: 0.4545\n",
      "Epoch 23/200\n",
      "11/11 - 0s - loss: 2.1335 - accuracy: 0.4545\n",
      "Epoch 24/200\n",
      "11/11 - 0s - loss: 2.1056 - accuracy: 0.3636\n",
      "Epoch 25/200\n",
      "11/11 - 0s - loss: 2.0770 - accuracy: 0.3636\n",
      "Epoch 26/200\n",
      "11/11 - 0s - loss: 2.0479 - accuracy: 0.3636\n",
      "Epoch 27/200\n",
      "11/11 - 0s - loss: 2.0186 - accuracy: 0.3636\n",
      "Epoch 28/200\n",
      "11/11 - 0s - loss: 1.9894 - accuracy: 0.3636\n",
      "Epoch 29/200\n",
      "11/11 - 0s - loss: 1.9609 - accuracy: 0.3636\n",
      "Epoch 30/200\n",
      "11/11 - 0s - loss: 1.9332 - accuracy: 0.3636\n",
      "Epoch 31/200\n",
      "11/11 - 0s - loss: 1.9069 - accuracy: 0.3636\n",
      "Epoch 32/200\n",
      "11/11 - 0s - loss: 1.8822 - accuracy: 0.3636\n",
      "Epoch 33/200\n",
      "11/11 - 0s - loss: 1.8592 - accuracy: 0.3636\n",
      "Epoch 34/200\n",
      "11/11 - 0s - loss: 1.8378 - accuracy: 0.3636\n",
      "Epoch 35/200\n",
      "11/11 - 0s - loss: 1.8179 - accuracy: 0.3636\n",
      "Epoch 36/200\n",
      "11/11 - 0s - loss: 1.7990 - accuracy: 0.3636\n",
      "Epoch 37/200\n",
      "11/11 - 0s - loss: 1.7806 - accuracy: 0.3636\n",
      "Epoch 38/200\n",
      "11/11 - 0s - loss: 1.7623 - accuracy: 0.3636\n",
      "Epoch 39/200\n",
      "11/11 - 0s - loss: 1.7434 - accuracy: 0.3636\n",
      "Epoch 40/200\n",
      "11/11 - 0s - loss: 1.7238 - accuracy: 0.4545\n",
      "Epoch 41/200\n",
      "11/11 - 0s - loss: 1.7033 - accuracy: 0.4545\n",
      "Epoch 42/200\n",
      "11/11 - 0s - loss: 1.6819 - accuracy: 0.4545\n",
      "Epoch 43/200\n",
      "11/11 - 0s - loss: 1.6599 - accuracy: 0.4545\n",
      "Epoch 44/200\n",
      "11/11 - 0s - loss: 1.6374 - accuracy: 0.4545\n",
      "Epoch 45/200\n",
      "11/11 - 0s - loss: 1.6148 - accuracy: 0.4545\n",
      "Epoch 46/200\n",
      "11/11 - 0s - loss: 1.5923 - accuracy: 0.4545\n",
      "Epoch 47/200\n",
      "11/11 - 0s - loss: 1.5700 - accuracy: 0.4545\n",
      "Epoch 48/200\n",
      "11/11 - 0s - loss: 1.5480 - accuracy: 0.4545\n",
      "Epoch 49/200\n",
      "11/11 - 0s - loss: 1.5264 - accuracy: 0.4545\n",
      "Epoch 50/200\n",
      "11/11 - 0s - loss: 1.5051 - accuracy: 0.4545\n",
      "Epoch 51/200\n",
      "11/11 - 0s - loss: 1.4839 - accuracy: 0.4545\n",
      "Epoch 52/200\n",
      "11/11 - 0s - loss: 1.4628 - accuracy: 0.4545\n",
      "Epoch 53/200\n",
      "11/11 - 0s - loss: 1.4416 - accuracy: 0.4545\n",
      "Epoch 54/200\n",
      "11/11 - 0s - loss: 1.4204 - accuracy: 0.4545\n",
      "Epoch 55/200\n",
      "11/11 - 0s - loss: 1.3991 - accuracy: 0.5455\n",
      "Epoch 56/200\n",
      "11/11 - 0s - loss: 1.3777 - accuracy: 0.5455\n",
      "Epoch 57/200\n",
      "11/11 - 0s - loss: 1.3565 - accuracy: 0.6364\n",
      "Epoch 58/200\n",
      "11/11 - 0s - loss: 1.3354 - accuracy: 0.6364\n",
      "Epoch 59/200\n",
      "11/11 - 0s - loss: 1.3146 - accuracy: 0.6364\n",
      "Epoch 60/200\n",
      "11/11 - 0s - loss: 1.2942 - accuracy: 0.6364\n",
      "Epoch 61/200\n",
      "11/11 - 0s - loss: 1.2741 - accuracy: 0.6364\n",
      "Epoch 62/200\n",
      "11/11 - 0s - loss: 1.2545 - accuracy: 0.6364\n",
      "Epoch 63/200\n",
      "11/11 - 0s - loss: 1.2352 - accuracy: 0.6364\n",
      "Epoch 64/200\n",
      "11/11 - 0s - loss: 1.2163 - accuracy: 0.6364\n",
      "Epoch 65/200\n",
      "11/11 - 0s - loss: 1.1977 - accuracy: 0.6364\n",
      "Epoch 66/200\n",
      "11/11 - 0s - loss: 1.1793 - accuracy: 0.6364\n",
      "Epoch 67/200\n",
      "11/11 - 0s - loss: 1.1612 - accuracy: 0.6364\n",
      "Epoch 68/200\n",
      "11/11 - 0s - loss: 1.1433 - accuracy: 0.6364\n",
      "Epoch 69/200\n",
      "11/11 - 0s - loss: 1.1257 - accuracy: 0.7273\n",
      "Epoch 70/200\n",
      "11/11 - 0s - loss: 1.1084 - accuracy: 0.7273\n",
      "Epoch 71/200\n",
      "11/11 - 0s - loss: 1.0914 - accuracy: 0.7273\n",
      "Epoch 72/200\n",
      "11/11 - 0s - loss: 1.0748 - accuracy: 0.7273\n",
      "Epoch 73/200\n",
      "11/11 - 0s - loss: 1.0584 - accuracy: 0.7273\n",
      "Epoch 74/200\n",
      "11/11 - 0s - loss: 1.0424 - accuracy: 0.7273\n",
      "Epoch 75/200\n",
      "11/11 - 0s - loss: 1.0266 - accuracy: 0.7273\n",
      "Epoch 76/200\n",
      "11/11 - 0s - loss: 1.0110 - accuracy: 0.7273\n",
      "Epoch 77/200\n",
      "11/11 - 0s - loss: 0.9956 - accuracy: 0.7273\n",
      "Epoch 78/200\n",
      "11/11 - 0s - loss: 0.9804 - accuracy: 0.7273\n",
      "Epoch 79/200\n",
      "11/11 - 0s - loss: 0.9653 - accuracy: 0.7273\n",
      "Epoch 80/200\n",
      "11/11 - 0s - loss: 0.9505 - accuracy: 0.7273\n",
      "Epoch 81/200\n",
      "11/11 - 0s - loss: 0.9359 - accuracy: 0.7273\n",
      "Epoch 82/200\n",
      "11/11 - 0s - loss: 0.9215 - accuracy: 0.7273\n",
      "Epoch 83/200\n",
      "11/11 - 0s - loss: 0.9074 - accuracy: 0.7273\n",
      "Epoch 84/200\n",
      "11/11 - 0s - loss: 0.8934 - accuracy: 0.7273\n",
      "Epoch 85/200\n",
      "11/11 - 0s - loss: 0.8797 - accuracy: 0.7273\n",
      "Epoch 86/200\n",
      "11/11 - 0s - loss: 0.8662 - accuracy: 0.8182\n",
      "Epoch 87/200\n",
      "11/11 - 0s - loss: 0.8528 - accuracy: 0.9091\n",
      "Epoch 88/200\n",
      "11/11 - 0s - loss: 0.8397 - accuracy: 0.9091\n",
      "Epoch 89/200\n",
      "11/11 - 0s - loss: 0.8268 - accuracy: 0.9091\n",
      "Epoch 90/200\n",
      "11/11 - 0s - loss: 0.8141 - accuracy: 0.9091\n",
      "Epoch 91/200\n",
      "11/11 - 0s - loss: 0.8016 - accuracy: 0.9091\n",
      "Epoch 92/200\n",
      "11/11 - 0s - loss: 0.7893 - accuracy: 0.9091\n",
      "Epoch 93/200\n",
      "11/11 - 0s - loss: 0.7772 - accuracy: 0.9091\n",
      "Epoch 94/200\n",
      "11/11 - 0s - loss: 0.7653 - accuracy: 0.9091\n",
      "Epoch 95/200\n",
      "11/11 - 0s - loss: 0.7537 - accuracy: 0.9091\n",
      "Epoch 96/200\n",
      "11/11 - 0s - loss: 0.7421 - accuracy: 0.9091\n",
      "Epoch 97/200\n",
      "11/11 - 0s - loss: 0.7308 - accuracy: 0.9091\n",
      "Epoch 98/200\n",
      "11/11 - 0s - loss: 0.7197 - accuracy: 0.9091\n",
      "Epoch 99/200\n",
      "11/11 - 0s - loss: 0.7087 - accuracy: 0.9091\n",
      "Epoch 100/200\n",
      "11/11 - 0s - loss: 0.6979 - accuracy: 0.9091\n",
      "Epoch 101/200\n",
      "11/11 - 0s - loss: 0.6873 - accuracy: 0.9091\n",
      "Epoch 102/200\n",
      "11/11 - 0s - loss: 0.6769 - accuracy: 0.9091\n",
      "Epoch 103/200\n",
      "11/11 - 0s - loss: 0.6665 - accuracy: 0.9091\n",
      "Epoch 104/200\n",
      "11/11 - 0s - loss: 0.6564 - accuracy: 0.9091\n",
      "Epoch 105/200\n",
      "11/11 - 0s - loss: 0.6463 - accuracy: 0.9091\n",
      "Epoch 106/200\n",
      "11/11 - 0s - loss: 0.6364 - accuracy: 0.9091\n",
      "Epoch 107/200\n",
      "11/11 - 0s - loss: 0.6267 - accuracy: 0.9091\n",
      "Epoch 108/200\n",
      "11/11 - 0s - loss: 0.6170 - accuracy: 0.9091\n",
      "Epoch 109/200\n",
      "11/11 - 0s - loss: 0.6075 - accuracy: 0.9091\n",
      "Epoch 110/200\n",
      "11/11 - 0s - loss: 0.5981 - accuracy: 0.9091\n",
      "Epoch 111/200\n",
      "11/11 - 0s - loss: 0.5888 - accuracy: 0.9091\n",
      "Epoch 112/200\n",
      "11/11 - 0s - loss: 0.5795 - accuracy: 0.9091\n",
      "Epoch 113/200\n",
      "11/11 - 0s - loss: 0.5704 - accuracy: 0.9091\n",
      "Epoch 114/200\n",
      "11/11 - 0s - loss: 0.5614 - accuracy: 0.9091\n",
      "Epoch 115/200\n",
      "11/11 - 0s - loss: 0.5525 - accuracy: 0.9091\n",
      "Epoch 116/200\n",
      "11/11 - 0s - loss: 0.5437 - accuracy: 0.9091\n",
      "Epoch 117/200\n",
      "11/11 - 0s - loss: 0.5349 - accuracy: 0.9091\n",
      "Epoch 118/200\n",
      "11/11 - 0s - loss: 0.5262 - accuracy: 0.9091\n",
      "Epoch 119/200\n",
      "11/11 - 0s - loss: 0.5177 - accuracy: 0.9091\n",
      "Epoch 120/200\n",
      "11/11 - 0s - loss: 0.5092 - accuracy: 0.9091\n",
      "Epoch 121/200\n",
      "11/11 - 0s - loss: 0.5007 - accuracy: 0.9091\n",
      "Epoch 122/200\n",
      "11/11 - 0s - loss: 0.4924 - accuracy: 0.9091\n",
      "Epoch 123/200\n",
      "11/11 - 0s - loss: 0.4841 - accuracy: 0.9091\n",
      "Epoch 124/200\n",
      "11/11 - 0s - loss: 0.4760 - accuracy: 0.9091\n",
      "Epoch 125/200\n",
      "11/11 - 0s - loss: 0.4679 - accuracy: 0.9091\n",
      "Epoch 126/200\n",
      "11/11 - 0s - loss: 0.4598 - accuracy: 0.9091\n",
      "Epoch 127/200\n",
      "11/11 - 0s - loss: 0.4519 - accuracy: 0.9091\n",
      "Epoch 128/200\n",
      "11/11 - 0s - loss: 0.4440 - accuracy: 0.9091\n",
      "Epoch 129/200\n",
      "11/11 - 0s - loss: 0.4362 - accuracy: 0.9091\n",
      "Epoch 130/200\n",
      "11/11 - 0s - loss: 0.4285 - accuracy: 0.9091\n",
      "Epoch 131/200\n",
      "11/11 - 0s - loss: 0.4209 - accuracy: 0.9091\n",
      "Epoch 132/200\n",
      "11/11 - 0s - loss: 0.4133 - accuracy: 0.9091\n",
      "Epoch 133/200\n",
      "11/11 - 0s - loss: 0.4058 - accuracy: 0.9091\n",
      "Epoch 134/200\n",
      "11/11 - 0s - loss: 0.3984 - accuracy: 0.9091\n",
      "Epoch 135/200\n",
      "11/11 - 0s - loss: 0.3911 - accuracy: 0.9091\n",
      "Epoch 136/200\n",
      "11/11 - 0s - loss: 0.3839 - accuracy: 0.9091\n",
      "Epoch 137/200\n",
      "11/11 - 0s - loss: 0.3768 - accuracy: 0.9091\n",
      "Epoch 138/200\n",
      "11/11 - 0s - loss: 0.3697 - accuracy: 0.9091\n",
      "Epoch 139/200\n",
      "11/11 - 0s - loss: 0.3628 - accuracy: 0.9091\n",
      "Epoch 140/200\n",
      "11/11 - 0s - loss: 0.3559 - accuracy: 0.9091\n",
      "Epoch 141/200\n",
      "11/11 - 0s - loss: 0.3491 - accuracy: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "11/11 - 0s - loss: 0.3425 - accuracy: 0.9091\n",
      "Epoch 143/200\n",
      "11/11 - 0s - loss: 0.3359 - accuracy: 0.9091\n",
      "Epoch 144/200\n",
      "11/11 - 0s - loss: 0.3294 - accuracy: 0.9091\n",
      "Epoch 145/200\n",
      "11/11 - 0s - loss: 0.3230 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "11/11 - 0s - loss: 0.3167 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "11/11 - 0s - loss: 0.3105 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "11/11 - 0s - loss: 0.3044 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "11/11 - 0s - loss: 0.2984 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "11/11 - 0s - loss: 0.2925 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "11/11 - 0s - loss: 0.2867 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "11/11 - 0s - loss: 0.2809 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "11/11 - 0s - loss: 0.2753 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "11/11 - 0s - loss: 0.2698 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "11/11 - 0s - loss: 0.2644 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "11/11 - 0s - loss: 0.2591 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "11/11 - 0s - loss: 0.2539 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "11/11 - 0s - loss: 0.2488 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "11/11 - 0s - loss: 0.2438 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "11/11 - 0s - loss: 0.2389 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "11/11 - 0s - loss: 0.2341 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "11/11 - 0s - loss: 0.2294 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "11/11 - 0s - loss: 0.2248 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "11/11 - 0s - loss: 0.2203 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "11/11 - 0s - loss: 0.2159 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "11/11 - 0s - loss: 0.2116 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "11/11 - 0s - loss: 0.2073 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "11/11 - 0s - loss: 0.2032 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "11/11 - 0s - loss: 0.1991 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "11/11 - 0s - loss: 0.1952 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "11/11 - 0s - loss: 0.1913 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "11/11 - 0s - loss: 0.1876 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "11/11 - 0s - loss: 0.1839 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "11/11 - 0s - loss: 0.1803 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "11/11 - 0s - loss: 0.1767 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "11/11 - 0s - loss: 0.1733 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "11/11 - 0s - loss: 0.1699 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "11/11 - 0s - loss: 0.1666 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "11/11 - 0s - loss: 0.1634 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "11/11 - 0s - loss: 0.1603 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "11/11 - 0s - loss: 0.1573 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "11/11 - 0s - loss: 0.1543 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "11/11 - 0s - loss: 0.1514 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "11/11 - 0s - loss: 0.1485 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "11/11 - 0s - loss: 0.1457 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "11/11 - 0s - loss: 0.1430 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "11/11 - 0s - loss: 0.1404 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "11/11 - 0s - loss: 0.1378 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "11/11 - 0s - loss: 0.1353 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "11/11 - 0s - loss: 0.1328 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "11/11 - 0s - loss: 0.1304 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "11/11 - 0s - loss: 0.1281 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "11/11 - 0s - loss: 0.1258 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "11/11 - 0s - loss: 0.1236 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "11/11 - 0s - loss: 0.1214 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "11/11 - 0s - loss: 0.1193 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "11/11 - 0s - loss: 0.1172 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "11/11 - 0s - loss: 0.1151 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "11/11 - 0s - loss: 0.1132 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "11/11 - 0s - loss: 0.1112 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c9fffc8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# y를 제거하였으므로 이제 X의 길이는...몰라.....\n",
    "# Embedding 두번째 인자는 임베딩 벡터의 출력 차원. 결과가 나오는 임베딩 벡터의 크기\n",
    "model.add(Embedding(vocab_size, 10, input_length=len(X[0])))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "        # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장\n",
    "        for word, index in t.word_index.items():\n",
    "            if index==result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측한 단어이므로 break\n",
    "            current_word = current_word + ' ' + word # 현재 단어 + ' ' + 예측한 단어를 현재 단어로 변경\n",
    "            sentence = sentence + ' ' + word # 예측한 단어를 문장에 저장\n",
    "        # for문이므로 이 행동을 다시 반복\n",
    "        sentence = init_word + sentence\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "말이\n"
     ]
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '말이', 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
